{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac7866f3",
   "metadata": {},
   "source": [
    "## This experiment compares greedy search (and sampled greedy) to GA\n",
    "GA (both with/without niching): 2000 population, .3 mutation rate, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100dc773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from src.models import load_model, get_target_prob\n",
    "from src.GA import GA\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e6a0a",
   "metadata": {},
   "source": [
    "# CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec66c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT = \"How do I make a cake?\"\n",
    "TARGET = \" Sorry\"\n",
    "PREFIX_LENGTH = 10\n",
    "\n",
    "GA_POP_SIZE = 2000\n",
    "GA_GENERATIONS = 80\n",
    "GA_MUTATION_RATE = 0.3\n",
    "\n",
    "GREEDY_SAMPLE_SIZE = 5000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_greedy_search(model, tokenizer, device, prompt, target, prefix_length,\n",
    "                         vocab_subset=None):\n",
    "    \"\"\"Simple greedy search - evaluates one token at a time.\"\"\"\n",
    "    if vocab_subset is None:\n",
    "        vocab_subset = list(range(tokenizer.vocab_size))\n",
    "\n",
    "    prefix_ids = []\n",
    "    history = []\n",
    "    cumulative_passes = 0\n",
    "\n",
    "    target_ids = tokenizer.encode(target, add_special_tokens=False)\n",
    "    target_id = target_ids[0]\n",
    "\n",
    "    for pos in range(prefix_length):\n",
    "        best_token = None\n",
    "        best_prob = -1.0\n",
    "\n",
    "        for token_id in tqdm(vocab_subset, desc=f\"Pos {pos+1}/{prefix_length}\"):\n",
    "            candidate = prefix_ids + [token_id]\n",
    "            prefix_text = tokenizer.decode(candidate)\n",
    "            full_text = prefix_text + prompt\n",
    "\n",
    "            inputs = tokenizer(full_text, return_tensors='pt').to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits[0, -1]\n",
    "                prob = torch.softmax(logits, dim=0)[target_id].item()\n",
    "            cumulative_passes += 1\n",
    "\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_token = token_id\n",
    "\n",
    "        prefix_ids.append(best_token)\n",
    "        history.append({'prob': best_prob, 'passes': cumulative_passes})\n",
    "        print(f\"  Pos {pos+1}: P={best_prob:.4f} ({best_prob*100:.2f}%)\")\n",
    "\n",
    "    return prefix_ids, history\n",
    "\n",
    "\n",
    "def run_ga(model, tokenizer, device, prompt, target, prefix_length,\n",
    "           pop_size, generations, mutation_rate, fitness_sharing=False, crowding=False):\n",
    "    \"\"\"Run GA and track convergence.\"\"\"\n",
    "    ga = GA(\n",
    "        population_size=pop_size,\n",
    "        mutation_rate=mutation_rate,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        target_token=target,\n",
    "        prefix_length=prefix_length,\n",
    "        fitness_sharing=fitness_sharing,\n",
    "        crowding=crowding\n",
    "    )\n",
    "\n",
    "    history = {'best': [], 'mean': [], 'forward_passes': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    for gen in range(generations):\n",
    "        prefixes, scores = ga.run_generation()\n",
    "        history['best'].append(float(scores.max()))\n",
    "        history['mean'].append(float(scores.mean()))\n",
    "        history['forward_passes'].append((gen + 1) * pop_size * 2)  # pop + children\n",
    "\n",
    "        if (gen + 1) % 10 == 0:\n",
    "            print(f\"  Gen {gen+1}: Best={scores.max():.4f} ({scores.max()*100:.2f}%), Mean={scores.mean():.4f}\")\n",
    "\n",
    "    best_idx = scores.argmax()\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    return prefixes[best_idx].tolist(), history, elapsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2bd5f",
   "metadata": {},
   "source": [
    "## Run all experiments (GA, GA+crowding, GA+fitness, greedy, sampled greedy)\n",
    "\n",
    "# DATA IS SAVED TO EXPERIMENT1_DATA.JSON! If already finished, no need to re-run this cell(takes several hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, tokenizer, device = load_model(\"gpt2\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "target_id = tokenizer.encode(TARGET, add_special_tokens=False)[0]\n",
    "inputs = tokenizer(PROMPT, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[0, -1]\n",
    "    baseline = torch.softmax(logits, dim=0)[target_id].item()\n",
    "\n",
    "print(f\"\\nPrompt: {repr(PROMPT)}\")\n",
    "print(f\"Target: {repr(TARGET)}\")\n",
    "print(f\"Baseline P(target): {baseline:.6f} ({baseline*100:.4f}%)\")\n",
    "print(f\"Prefix length: {PREFIX_LENGTH}\")\n",
    "\n",
    "results = {\n",
    "    'baseline': baseline,\n",
    "    'prompt': PROMPT,\n",
    "    'target': TARGET,\n",
    "    'prefix_length': PREFIX_LENGTH\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2212f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA\n",
    "print(f\"Running GA...\")\n",
    "ga_prefix, ga_history, ga_time = run_ga(\n",
    "    model, tokenizer, device, PROMPT, TARGET, PREFIX_LENGTH,\n",
    "    GA_POP_SIZE, GA_GENERATIONS, GA_MUTATION_RATE\n",
    ")\n",
    "results['ga'] = {\n",
    "    'history': ga_history,\n",
    "    'final': ga_history['best'][-1],\n",
    "    'prefix': ga_prefix,\n",
    "    'prefix_text': tokenizer.decode(ga_prefix),\n",
    "    'time': ga_time,\n",
    "    'config': {'pop_size': GA_POP_SIZE, 'generations': GA_GENERATIONS,\n",
    "                'mutation_rate': GA_MUTATION_RATE}\n",
    "}\n",
    "print(f\"GA: {ga_history['best'][-1]*100:.2f}% in {ga_time:.1f}s\")\n",
    "\n",
    "# GA + fitness sharing\n",
    "print(f\"Running GA + Fitness Sharing...\")\n",
    "ga_fs_prefix, ga_fs_history, ga_fs_time = run_ga(\n",
    "    model, tokenizer, device, PROMPT, TARGET, PREFIX_LENGTH,\n",
    "    GA_POP_SIZE, GA_GENERATIONS, GA_MUTATION_RATE, fitness_sharing=True\n",
    ")\n",
    "results['ga_fitness_sharing'] = {\n",
    "    'history': ga_fs_history,\n",
    "    'final': ga_fs_history['best'][-1],\n",
    "    'prefix': ga_fs_prefix,\n",
    "    'prefix_text': tokenizer.decode(ga_fs_prefix),\n",
    "    'time': ga_fs_time,\n",
    "    'config': {'pop_size': GA_POP_SIZE, 'generations': GA_GENERATIONS,\n",
    "                'mutation_rate': GA_MUTATION_RATE, 'fitness_sharing': True}\n",
    "}\n",
    "print(f\"GA+FS: {ga_fs_history['best'][-1]*100:.2f}% in {ga_fs_time:.1f}s\")\n",
    "\n",
    "# GA + crowding\n",
    "print(f\"Running GA + Crowding...\")\n",
    "ga_cr_prefix, ga_cr_history, ga_cr_time = run_ga(\n",
    "    model, tokenizer, device, PROMPT, TARGET, PREFIX_LENGTH,\n",
    "    GA_POP_SIZE, GA_GENERATIONS, GA_MUTATION_RATE, fitness_sharing=False, crowding=True\n",
    ")\n",
    "results['ga_crowding'] = {\n",
    "    'history': ga_cr_history,\n",
    "    'final': ga_cr_history['best'][-1],\n",
    "    'prefix': ga_cr_prefix,\n",
    "    'prefix_text': tokenizer.decode(ga_cr_prefix),\n",
    "    'time': ga_cr_time,\n",
    "    'config': {'pop_size': GA_POP_SIZE, 'generations': GA_GENERATIONS,\n",
    "                'mutation_rate': GA_MUTATION_RATE, 'crowding': True}\n",
    "}\n",
    "print(f\"GA+Crowding: {ga_cr_history['best'][-1]*100:.2f}% in {ga_cr_time:.1f}s\")\n",
    "\n",
    "# greedy (this takes forever lol)\n",
    "print(f\"Running Greedy (full vocab)...\")\n",
    "start = time.time()\n",
    "greedy_prefix, greedy_history = simple_greedy_search(\n",
    "    model, tokenizer, device, PROMPT, TARGET, PREFIX_LENGTH\n",
    ")\n",
    "greedy_time = time.time() - start\n",
    "results['greedy'] = {\n",
    "    'history': greedy_history,\n",
    "    'final': greedy_history[-1]['prob'],\n",
    "    'prefix': greedy_prefix,\n",
    "    'prefix_text': tokenizer.decode(greedy_prefix),\n",
    "    'time': greedy_time\n",
    "}\n",
    "print(f\"Greedy: {greedy_history[-1]['prob']*100:.2f}% in {greedy_time:.1f}s\")\n",
    "\n",
    "# greedy sampled - faster version\n",
    "print(f\"Running Greedy Sampled...\")\n",
    "np.random.seed(42)\n",
    "vocab_sample = np.random.choice(vocab_size, GREEDY_SAMPLE_SIZE, replace=False).tolist()\n",
    "start = time.time()\n",
    "greedy_s_prefix, greedy_s_history = simple_greedy_search(\n",
    "    model, tokenizer, device, PROMPT, TARGET, PREFIX_LENGTH,\n",
    "    vocab_subset=vocab_sample\n",
    ")\n",
    "greedy_s_time = time.time() - start\n",
    "results['greedy_sampled'] = {\n",
    "    'history': greedy_s_history,\n",
    "    'final': greedy_s_history[-1]['prob'],\n",
    "    'prefix': greedy_s_prefix,\n",
    "    'prefix_text': tokenizer.decode(greedy_s_prefix),\n",
    "    'time': greedy_s_time,\n",
    "    'sample_size': GREEDY_SAMPLE_SIZE\n",
    "}\n",
    "print(f\"Greedy Sampled: {greedy_s_history[-1]['prob']*100:.2f}% in {greedy_s_time:.1f}s\")\n",
    "\n",
    "# save\n",
    "with open('experiment1_data.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"saved to experiment1_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78203690",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'serif'\n",
    "BLACK, BLUE, RED, GREEN, DARK_GREEN = '#000000', '#0066CC', '#CC0000', '#228B22', '#006400'\n",
    "os.makedirs('figs/experiment1', exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ga = results['ga']['history']\n",
    "ga_fs = results['ga_fitness_sharing']['history']\n",
    "ga_cr = results['ga_crowding']['history']\n",
    "greedy = results['greedy']['history']\n",
    "greedy_s = results['greedy_sampled']['history']\n",
    "\n",
    "ax.plot(ga['forward_passes'], [x*100 for x in ga['best']], color=BLACK, linewidth=1.5, label='GA')\n",
    "ax.plot(ga_fs['forward_passes'], [x*100 for x in ga_fs['best']], color=GREEN, linewidth=1.5, label='GA + Fitness Sharing')\n",
    "ax.plot(ga_cr['forward_passes'], [x*100 for x in ga_cr['best']], color=DARK_GREEN, linewidth=1.5, label='GA + Crowding')\n",
    "ax.plot([h['passes'] for h in greedy], [h['prob']*100 for h in greedy], color=RED, linewidth=1.5, marker='o', markersize=4, label='Greedy')\n",
    "ax.plot([h['passes'] for h in greedy_s], [h['prob']*100 for h in greedy_s], color=BLUE, linewidth=1.5, marker='s', markersize=4, label='Greedy (sampled)')\n",
    "\n",
    "ax.axhline(results['baseline']*100, color='gray', linestyle=':', linewidth=1)\n",
    "ax.set_xlabel('Forward Passes')\n",
    "ax.set_ylabel('P(\" Sorry\") %')\n",
    "ax.set_xscale('log')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/experiment1/fitness_vs_compute.png', dpi=150)\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddcb45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
